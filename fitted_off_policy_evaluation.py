import torch
from fitted_algo import FittedAlgo
import numpy as np
from tqdm import tqdm
from env_nn import *
from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
from operator import add 



class PortfolioFittedQEvaluation(FittedAlgo):
    def __init__(self, state_space_dim, 
                       dim_of_actions, 
                       max_epochs, 
                       gamma, 
                       model_type='cnn', 
                       num_frame_stack=None):

        '''
        An implementation of fitted Q iteration

        num_inputs: number of inputs
        dim_of_actions: dimension of action space
        max_epochs: positive int, specifies how many iterations to run the algorithm
        gamma: discount factor
        '''
        self.model_type = model_type


        self.state_space_dim = state_space_dim
        self.action_space_dim = dim_of_actions
        self.max_epochs = max_epochs
        self.gamma = gamma
        self.num_frame_stack = num_frame_stack
        self.Q_k = None
        self.Q_k_minus_1 = None

        earlyStopping = EarlyStopping(monitor='val_loss', min_delta=1e-4,  patience=10, verbose=1, mode='min', restore_best_weights=True)
        mcp_save = ModelCheckpoint('fqi.hdf5', save_best_only=True, monitor='val_loss', mode='min')
        reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=7, verbose=1, min_delta=1e-4, mode='min')

        self.more_callbacks = [earlyStopping, mcp_save, reduce_lr_loss]

        super(PortfolioFittedQEvaluation, self).__init__()

    def run(self, policy, which_cost, dataset, epochs=1, epsilon=1e-8, desc='FQE', g_idx=None, testing=True, **kw):
        # dataset is the original dataset generated by pi_{old} to which we will find
        # an approximately optimal Q
#        print(which_cost)
        dataset.set_cost(which_cost, idx=g_idx)
#        print ('Scale: ', dataset.scale)
        # try:
        #     initial_states = np.unique([episode.frames[[0]*episode.num_frame_stack] for episode in dataset.episodes], axis=0)
        # except:
        #     initial_states = np.rollaxis(dataset['frames'][dataset['prev_states'][[0]]],1,4)

#        initial_states = np.rollaxis(dataset['frames'][dataset['prev_states'][[0]]],1,4)
        initial_states = dataset['prev_states']

        # if self.Q_k is None:
        self.Q_k = self.init_Q(**kw)
#        self.Q_k_minus_1 = self.init_Q(model_type=self.model_type, num_frame_stack=self.num_frame_stack, **kw)
#        x_prime = np.rollaxis(dataset['frames'][dataset['next_states'][[0]]], 1,4)
#        self.Q_k.min_over_a([x_prime], x_preprocessed=True)[0]
#        self.Q_k_minus_1.min_over_a([x_prime], x_preprocessed=True)[0]
#        self.Q_k.copy_over_to(self.Q_k_minus_1)
        values = []

        for k in tqdm(range(self.max_epochs), desc=desc):
            print("epoch:",k)
            batch_size = 256
            
            dataset_length = len(dataset)
            perm = np.random.permutation(range(dataset_length))
            eighty_percent_of_set = int(1.*len(perm))
            training_idxs = perm[:eighty_percent_of_set]
            validation_idxs = perm[eighty_percent_of_set:]
            training_steps_per_epoch = int(.3 * np.ceil(len(training_idxs)/float(batch_size)))
            validation_steps_per_epoch = int(np.ceil(len(validation_idxs)/float(batch_size)))
            # steps_per_epoch = 1 #int(np.ceil(len(dataset)/float(batch_size)))
            train_gen = self.generator(policy, dataset, training_idxs, fixed_permutation=True, batch_size=batch_size)
            # val_gen = self.generator(policy, dataset, validation_idxs, fixed_permutation=True, batch_size=batch_size)
            
            self.fit_generator(train_gen, 
                               model_params = self.Q_k.parameters(),
                               steps_per_epoch=training_steps_per_epoch,
                               #validation_data=val_gen, 
                               #validation_steps=validation_steps_per_epoch,
                               epochs=epochs, 
#                               max_queue_size=10, 
#                               workers=4, 
#                               use_multiprocessing=False, 
#                               epsilon=epsilon, 
                               evaluate=False, 
                               verbose=0,
                               additional_callbacks = self.more_callbacks)
#            self.Q_k.copy_over_to(self.Q_k_minus_1)
#            print(type(policy))
            if testing:
                _,actions = policy.min_over_a_cont(initial_states)
                assert len(actions) == len(initial_states)
                Q_val=[]
                for i in range(len(initial_states)):
                    X_p = torch.FloatTensor(initial_states[i]).view(-1,self.state_space_dim[0],5,4)
                    act = torch.FloatTensor([actions[i]])
#                    print(X_p.shape,act.shape)
                    Q = self.Q_k.forward(X_p,act)
#                    print(X_p.shape,act.shape,Q)
                    Q_val.append(Q.detach().numpy())
#                Q_val = self.Q_k.forward(initial_states,actions)
#                print("Q_val",Q_val)
#                values.append(np.mean(Q_val)*dataset.scale)
                values.append(np.mean(Q_val))

        # initial_states = self.Q_k.representation(initial_states)
        if testing:
#            print(values,values[-10:])
            return np.mean(values[-10:]), values
        
        Q_val=[]
        for i in range(len(initial_states)):
            X_p = torch.FloatTensor(initial_states[i]).view(-1,self.state_space_dim[0],5,4)
            act = torch.FloatTensor([actions[i]])
#                    print(X_p.shape,act.shape)
            Q = self.Q_k.forward(X_p,act)
            Q_val.append(Q.detach().numpy())
#        actions = policy(initial_states[:,np.newaxis,...], x_preprocessed=True)
#        Q_val = self.Q_k.all_actions([initial_states], x_preprocessed=True)[np.arange(len(actions)), actions]
#        print("Q_val",np.mean(Q_val),dataset.scale)
#        return np.mean(Q_val)*dataset.scale, values
        return np.mean(Q_val), values

#    @threadsafe_generator
    def generator(self, policy, dataset, training_idxs, fixed_permutation=False,  batch_size = 64):
        data_length = len(training_idxs)
        steps = int(np.ceil(data_length/float(batch_size)))
        i = -1
        amount_of_data_calcd = 0
        if fixed_permutation:
            calcd_costs = np.empty((len(training_idxs),), dtype='float64')
        while i<steps-1:
            i = (i + 1) % steps
            if fixed_permutation:
                if i == 0: perm = np.random.permutation(training_idxs)
                batch_idxs = perm[(i*batch_size):((i+1)*batch_size)]
            else:
                batch_idxs = np.random.choice(training_idxs, batch_size)
            X=[dataset['prev_states'][i] for i in batch_idxs]
            actions = [np.atleast_2d(dataset['a'][i]).T for i in batch_idxs]
            x_prime = [dataset['next_states'][i] for i in batch_idxs]
            dataset_costs = [dataset['cost'][i] for i in batch_idxs]
            dones = [dataset['done'][i] for i in batch_idxs]
            policy_action =  [dataset['pi_of_x_prime'][i] for i in batch_idxs]

            # if fixed_permutation:
            #     if amount_of_data_calcd <= data_length:
            #         costs = dataset_costs + self.gamma*self.Q_k_minus_1.min_over_a([x_prime], x_preprocessed=True)[0]*(1-dones.astype(int))
            #         calcd_costs[(i*batch_size):((i+1)*batch_size)] = costs
            #     else:
            #         costs = calcd_costs[(i*batch_size):((i+1)*batch_size)]
            # else:
            # policy_action = policy(x_prime[:,np.newaxis,...], x_preprocessed=True)
#            Q_val = self.Q_k_minus_1.all_actions([x_prime], x_preprocessed=True)[np.arange(len(policy_action)), policy_action]
            Q_val=[]
            for i in range(len(x_prime)):
                X_p = torch.FloatTensor(x_prime[i]).view(-1,self.state_space_dim[0],5,4)
                act = torch.FloatTensor([policy_action[i]])
                Q = self.Q_k.forward(X_p,act)
                Q_val.append(Q.detach().numpy())
            costs = list(map(add, dataset_costs, [self.gamma*Q*(1-int(x)) for Q,x in zip(Q_val,dones)]))
#            costs = dataset_costs + (self.gamma*Q_val*(1-dones.astype(int))).reshape(-1)

#            X = self.Q_k_minus_1.representation([X], actions, x_preprocessed=True)

            yield (X,actions, costs)

    def init_Q(self, epsilon=1e-10, **kw):
        return PortfolioNN_model(self.state_space_dim, self.action_space_dim,n_epochs=10,**kw)



